"use strict";(self.webpackChunk_mlflow_mlflow=self.webpackChunk_mlflow_mlflow||[]).push([[5813],{177:function(e,t,n){function o(e){return[{key:"LAST_HOUR",label:e.formatMessage({id:"bjoGjg",defaultMessage:"Last hour"})},{key:"LAST_24_HOURS",label:e.formatMessage({id:"JChRnq",defaultMessage:"Last 24 hours"})},{key:"LAST_7_DAYS",label:e.formatMessage({id:"eQsXm7",defaultMessage:"Last 7 days"})},{key:"LAST_30_DAYS",label:e.formatMessage({id:"p1H1KR",defaultMessage:"Last 30 days"})},{key:"LAST_YEAR",label:e.formatMessage({id:"EI7moF",defaultMessage:"Last year"})},{key:"ALL",label:e.formatMessage({id:"HeNa8H",defaultMessage:"All"})},{key:"CUSTOM",label:e.formatMessage({id:"TpmJlu",defaultMessage:"Custom"})}]}n.d(t,{p:function(){return o}})},26022:function(e,t,n){n.d(t,{UY:function(){return m},VA:function(){return c},oy:function(){return d},vF:function(){return s}});var o=n(31014),a=n(48624),i=n(61197);const s="startTimeLabel",l="startTime",r="endTime",d="LAST_7_DAYS",c=()=>{var e;const t=(0,i.U)(),[n,c]=(0,a.ok)(),u=n.get(s)||d;let g=n.get(l)||void 0,p=null!==(e=n.get(r))&&void 0!==e?e:void 0;if("CUSTOM"!==u){const e=m(t.dateNow,{startTimeLabel:u});g=e.startTime,p=e.endTime}else{var h;g=n.get(l)||void 0,p=null!==(h=n.get(r))&&void 0!==h?h:void 0}const f=(0,o.useMemo)((()=>({startTimeLabel:u,startTime:g,endTime:p})),[u,g,p]),T=(0,o.useCallback)(((e,t=!1)=>{c((t=>(void 0===(null===e||void 0===e?void 0:e.startTime)?t.delete(l):"CUSTOM"===e.startTimeLabel&&t.set(l,e.startTime),void 0===(null===e||void 0===e?void 0:e.endTime)?t.delete(r):"CUSTOM"===e.startTimeLabel&&t.set(r,e.endTime),void 0===(null===e||void 0===e?void 0:e.startTimeLabel)?t.delete(s):t.set(s,e.startTimeLabel),t)),{replace:t})}),[c]);return[f,T]};function m(e,t){return t.startTimeLabel&&"CUSTOM"!==t.startTimeLabel?function(e,t){switch(t){case"LAST_HOUR":return{startTime:new Date(new Date(e).setUTCHours((new Date).getUTCHours()-1)).toISOString(),endTime:e.toISOString()};case"LAST_24_HOURS":return{startTime:new Date(new Date(e).setUTCDate((new Date).getUTCDate()-1)).toISOString(),endTime:e.toISOString()};case"LAST_7_DAYS":return{startTime:new Date(new Date(e).setUTCDate((new Date).getUTCDate()-7)).toISOString(),endTime:e.toISOString()};case"LAST_30_DAYS":return{startTime:new Date(new Date(e).setUTCDate((new Date).getUTCDate()-30)).toISOString(),endTime:e.toISOString()};case"LAST_YEAR":return{startTime:new Date(new Date(e).setUTCFullYear((new Date).getUTCFullYear()-1)).toISOString(),endTime:e.toISOString()};case"ALL":return{startTime:void 0,endTime:e.toISOString()};default:throw new Error(`Unexpected start time label: ${t}`)}}(e,t.startTimeLabel):{startTime:t.startTime,endTime:t.endTime}}},36432:function(e,t,n){n.d(t,{i:function(){return y}});var o=n(89555),a=n(16934),i=n(68048),s=n(88443),l=n(9133),r=n(95451),d=n(56412),c=(n(31014),n(50111));var m={name:"ddxhyk",styles:"max-width:800px"},u={name:"ddxhyk",styles:"max-width:800px"};const g={openai:{minVersion:"2.15.1",getContent:()=>(0,c.Y)(s.A,{id:"smtq2M",defaultMessage:"Automatically log traces for OpenAI API calls by calling the {code} function. For example:",values:{code:(0,c.Y)("code",{children:"mlflow.openai.autolog()"})}}),getCodeSource:()=>'from openai import OpenAI\n\nmlflow.openai.autolog()\n\n# Ensure that the "OPENAI_API_KEY" environment variable is set\nclient = OpenAI()\n\nmessages = [\n  {"role": "system", "content": "You are a helpful assistant."},\n  {"role": "user", "content": "Hello!"}\n]\n\n# Inputs and outputs of the API request will be logged in a trace\nclient.chat.completions.create(model="gpt-4o-mini", messages=messages)'},langchain:{minVersion:"2.17.2",getContent:()=>(0,c.Y)(s.A,{id:"7/urtn",defaultMessage:"Automatically log traces for LangChain or LangGraph invocations by calling the {code} function. For example:",values:{code:(0,c.Y)("code",{children:"mlflow.langchain.autolog()"})}}),getCodeSource:()=>'from langchain_openai import OpenAI\nfrom langchain_core.prompts import PromptTemplate\n\nmlflow.langchain.autolog()\n\n# Ensure that the "OPENAI_API_KEY" environment variable is set\nllm = OpenAI()\nprompt = PromptTemplate.from_template("Answer the following question: {question}")\nchain = prompt | llm\n\n# Invoking the chain will cause a trace to be logged\nchain.invoke("What is MLflow?")'},llama_index:{minVersion:"2.15.1",getContent:()=>(0,c.Y)(s.A,{id:"/v6KuF",defaultMessage:"Automatically log traces for LlamaIndex queries by calling the {code} function. For example:",values:{code:(0,c.Y)("code",{children:"mlflow.llama_index.autolog()"})}}),getCodeSource:()=>'from llama_index.core import Document, VectorStoreIndex\n\nmlflow.llama_index.autolog()\n\n# Ensure that the "OPENAI_API_KEY" environment variable is set\nindex = VectorStoreIndex.from_documents([Document.example()])\nquery_engine = index.as_query_engine()\n\n# Querying the engine will cause a trace to be logged\nquery_engine.query("What is LlamaIndex?")'},dspy:{minVersion:"2.18.0",getContent:()=>(0,c.Y)(s.A,{id:"h1HQ14",defaultMessage:"Automatically log traces for DSPy executions by calling the {code} function. For example:",values:{code:(0,c.Y)("code",{children:"mlflow.dspy.autolog()"})}}),getCodeSource:()=>'import dspy\n\nmlflow.dspy.autolog()\n\n# Configure the LLM to use. Please ensure that\n# the OPENAI_API_KEY environment variable is set\nlm = dspy.LM("openai/gpt-4o-mini")\ndspy.configure(lm=lm)\n\n# Define a simple chain-of-thought model and run it\nmath = dspy.ChainOfThought("question -> answer: float")\nquestion = "Two dice are tossed. What is the probability that the sum equals two?"\n\n# All intermediate outputs from the execution will be logged\nmath(question=question)'},crewai:{minVersion:"2.19.0",getContent:()=>(0,c.Y)(s.A,{id:"K8LdMX",defaultMessage:"Automatically log traces for CrewAI executions by calling the {code} function. For example:",values:{code:(0,c.Y)("code",{children:"mlflow.crewai.autolog()"})}}),getCodeSource:()=>'from crewai import Agent, Crew, Process, Task\n\nmlflow.crewai.autolog()\n\ncity_selection_agent = Agent(\n    role="City selection expert",\n    goal="Select the best city based on weather, season, and prices",\n    backstory="An expert in analyzing travel data to pick ideal destinations",\n    allow_delegation=True,\n    verbose=True,\n)\n\nlocal_expert = Agent(\n    role="Local expert",\n    goal="Provide the best insights about the selected city",\n    backstory="A local guide with extensive information about the city",\n    verbose=True,\n)\n  \nplan_trip = Task(\n    name="Plan a trip",\n    description="""Plan a trip to a city based on weather, prices, and best local attractions. \n    Please consult with a local expert when researching things to do.""",\n    expected_output="A short summary of the trip destination and key things to do",\n    agent=city_selection_agent,\n)\n\ncrew = Crew(\n  agents=[\n    city_selection_agent,\n    local_expert,\n  ],\n  tasks=[plan_trip],\n  process=Process.sequential\n)\n\n# Ensure the "OPENAI_API_KEY" environment variable is set\n# before kicking off the crew. All intermediate agent outputs\n# will be logged in the resulting trace.\ncrew.kickoff()'},autogen:{minVersion:"2.16.2",getContent:()=>(0,c.Y)(s.A,{id:"i/pJvo",defaultMessage:"Automatically log traces for AutoGen conversations by calling the {code} function. For example:",values:{code:(0,c.Y)("code",{children:"mlflow.autogen.autolog()"})}}),getCodeSource:()=>'import os\nfrom autogen import AssistantAgent, UserProxyAgent\n\nmlflow.autogen.autolog()\n\n# Ensure that the "OPENAI_API_KEY" environment variable is set\nllm_config = { "model": "gpt-4o-mini", "api_key": os.environ["OPENAI_API_KEY"] }\nassistant = AssistantAgent("assistant", llm_config = llm_config)\nuser_proxy = UserProxyAgent("user_proxy", code_execution_config = False)\n\n# All intermediate executions within the chat session will be logged\nuser_proxy.initiate_chat(assistant, message = "What is MLflow?", max_turns = 1)'},anthropic:{minVersion:"2.19.0",getContent:()=>(0,c.Y)(s.A,{id:"AzOnmT",defaultMessage:"Automatically log traces for Anthropic API calls by calling the {code} function. For example:",values:{code:(0,c.Y)("code",{children:"mlflow.anthropic.autolog()"})}}),getCodeSource:()=>'import os\nimport anthropic\n\n# Enable auto-tracing for Anthropic\nmlflow.anthropic.autolog()\n\n# Configure your API key (please ensure that the "ANTHROPIC_API_KEY" environment variable is set)\nclient = anthropic.Anthropic(api_key=os.environ["ANTHROPIC_API_KEY"])\n\n# Inputs and outputs of API calls will be logged as a trace\nmessage = client.messages.create(\n    model="claude-3-5-sonnet-20241022",\n    max_tokens=1024,\n    messages=[\n        {"role": "user", "content": "Hello, Claude"},\n    ],\n)'},bedrock:{minVersion:"2.20.0",getContent:()=>(0,c.Y)(s.A,{id:"6tKW1I",defaultMessage:"Automatically log traces for Bedrock conversations by calling the {code} function. For example:",values:{code:(0,c.Y)("code",{children:"mlflow.bedrock.autolog()"})}}),getCodeSource:()=>'import boto3\n\nmlflow.bedrock.autolog()\n\n# Ensure that your boto3 client has the necessary auth information\nbedrock = boto3.client(\n    service_name="bedrock-runtime",\n    region_name="<REPLACE_WITH_YOUR_AWS_REGION>",\n)\n\nmodel = "anthropic.claude-3-5-sonnet-20241022-v2:0"\nmessages = [{ "role": "user", "content": [{"text": "Hello!"}]}]\n\n# All intermediate executions within the chat session will be logged\nbedrock.converse(modelId=model, messages=messages)'},litellm:{minVersion:"2.18.0",getContent:()=>(0,c.Y)(s.A,{id:"D7SSDK",defaultMessage:"Automatically log traces for LiteLLM API calls by calling the {code} function. For example:",values:{code:(0,c.Y)("code",{children:"mlflow.litellm.autolog()"})}}),getCodeSource:()=>'import litellm\n\nmlflow.litellm.autolog()\n\n# Ensure that the "OPENAI_API_KEY" environment variable is set\nmessages = [{"role": "user", "content": "Hello!"}]\n\n# Inputs and outputs of the API request will be logged in a trace\nlitellm.completion(model="gpt-4o-mini", messages=messages)'},gemini:{minVersion:"2.18.0",getContent:()=>(0,c.Y)(s.A,{id:"IsIgE2",defaultMessage:"Automatically log traces for Gemini conversations by calling the {code} function. For example:",values:{code:(0,c.Y)("code",{children:"mlflow.gemini.autolog()"})}}),getCodeSource:()=>'import google.genai as genai\n\nmlflow.gemini.autolog()\n\n# Replace "GEMINI_API_KEY" with your API key\nclient = genai.Client(api_key="GEMINI_API_KEY")\n\n# Inputs and outputs of the API request will be logged in a trace\nclient.models.generate_content(model="gemini-1.5-flash", contents="Hello!")'},custom:{minVersion:"2.14.3",getContent:e=>(0,c.FD)(c.FK,{children:[(0,c.Y)(a.T.Paragraph,{css:m,children:(0,c.Y)(s.A,{id:"3Z6K+n",defaultMessage:"To manually instrument your own traces, the most convenient method is to use the {code} function decorator. This will cause the inputs and outputs of the function to be captured in the trace.",values:{code:(0,c.Y)("code",{children:"@mlflow.trace"})}})}),(0,c.Y)(a.T.Paragraph,{css:u,children:(0,c.Y)(s.A,{id:"WNz02j",defaultMessage:"For more complex use cases, MLflow also provides granular APIs that can be used to control tracing behavior. For more information, please visit the <a>official documentation</a> on fluent and client APIs for MLflow Tracing.",values:{a:t=>(0,c.Y)(a.T.Link,{title:"official documentation",componentId:`${e}.traces_table.custom_tracing_docs_link`,href:"https://mlflow.org/docs/latest/llms/tracing/index.html#tracing-fluent-apis",openInNewTab:!0,children:t})}})})]}),getCodeSource:()=>'@mlflow.trace\ndef foo(a):\nreturn a + bar(a)\n\n# Various attributes can be passed to the decorator\n# to modify the information contained in the span\n@mlflow.trace(name = "custom_name", attributes = { "key": "value" })\ndef bar(b):\nreturn b + 1\n\n# Invoking the traced function will cause a trace to be logged\nfoo(1)'}};var p=n(51455);var h={name:"ddxhyk",styles:"max-width:800px"},f={name:"1hyob9y",styles:"position:relative;width:min-content"};const T=({flavorName:e,baseComponentId:t})=>{const{theme:n}=(0,a.u)(),{getContent:l,getCodeSource:m,minVersion:u}=g[e],{displayVersionWarnings:T=!0}=(0,p.S)(),y=l(t),A=`import mlflow\nfrom packaging.version import Version\n\nassert Version(mlflow.__version__) >= Version("${u}"), (\n  "This feature requires MLflow version ${u} or newer. "\n  "Please run '%pip install -U mlflow' in a notebook cell, "\n  "and restart the kernel when the command finishes."\n)\n\n`+m(),w=(0,c.Y)(s.A,{id:"YfspJh",defaultMessage:"This example requires MLflow version {minVersion} or newer. Please run {installCommand} in a notebook cell if your MLflow version is older than this, and restart the kernel when the command finishes.",values:{minVersion:u,installCommand:(0,c.Y)(a.T.Text,{code:!0,children:"%pip install -U mlflow"})}});return(0,c.FD)("div",{children:[T&&(0,c.Y)(i.Alert,{componentId:`${t}.traces_table.${e}_quickstart_alert`,css:(0,o.AH)({marginBottom:n.spacing.md,maxWidth:800},""),closable:!1,message:(0,c.Y)(s.A,{id:"NoYMjZ",defaultMessage:"Requires MLflow >= {minVersion}",values:{minVersion:u}}),description:w,type:"info"}),(0,c.Y)(a.T.Text,{css:h,children:y}),(0,c.FD)("div",{css:f,children:[(0,c.Y)(d.i,{componentId:`${t}.traces_table.${e}_quickstart_snippet_copy`,css:(0,o.AH)({zIndex:1,position:"absolute",top:n.spacing.xs,right:n.spacing.xs},""),showLabel:!1,copyText:A,icon:(0,c.Y)(i.CopyIcon,{})}),(0,c.Y)(r.z7,{showLineNumbers:!0,theme:n.isDarkMode?"duotoneDark":"light",style:{padding:`${n.spacing.sm}px ${n.spacing.md}px`,marginTop:n.spacing.md},language:"python",children:A})]})]})},y=({baseComponentId:e,runUuid:t})=>{const{theme:n}=(0,a.u)(),{introductionText:r}=(0,p.S)();return(0,c.FD)("div",{css:(0,o.AH)({overflow:"auto",paddingBottom:n.spacing.lg},""),children:[(0,c.Y)(i.Header,{title:(0,c.Y)(s.A,{id:"6d5JTO",defaultMessage:"No traces recorded"}),titleElementLevel:3}),(0,c.Y)(a.T.Text,{css:(0,o.AH)({display:"block",marginTop:n.spacing.md,marginBottom:n.spacing.md,maxWidth:800},""),children:r||(0,c.Y)(s.A,{id:"msYDmK",defaultMessage:"This tab displays all the traces logged to this {isRun, select, true {run} other {experiment}}. Follow the steps below to log your first trace. For more information about MLflow Tracing, visit the <a>MLflow documentation</a>.",values:{isRun:!(0,l.isNil)(t),a:t=>(0,c.Y)(a.T.Link,{componentId:`${e}.traces_table.quickstart_docs_link`,href:"https://mlflow.org/docs/latest/llms/tracing/index.html",openInNewTab:!0,children:t})}})}),(0,c.Y)(T,{flavorName:"custom",baseComponentId:e})]})}},51455:function(e,t,n){n.d(t,{A:function(){return s},S:function(){return l}});var o=n(31014),a=n(50111);const i=(0,o.createContext)({}),s=({children:e,introductionText:t,displayVersionWarnings:n})=>(0,a.Y)(i.Provider,{value:{introductionText:t,displayVersionWarnings:n},children:e}),l=()=>(0,o.useContext)(i)},61197:function(e,t,n){n.d(t,{U:function(){return d},k:function(){return r}});var o=n(9133),a=n(31014),i=n(50111);const s=()=>({dateNow:new Date,lastRefreshTime:Date.now(),refresh:()=>{}}),l=(0,a.createContext)(s()),r=({config:e,children:t})=>{const n=s(),r=(0,o.merge)({},n,e),[d,c]=a.useState(r.lastRefreshTime),m=(0,a.useMemo)((()=>new Date(d)),[d]),u=(0,a.useCallback)((()=>{c(Date.now())}),[]);return(0,i.Y)(l.Provider,{value:{...r,dateNow:m,lastRefreshTime:d,refresh:u},children:t})},d=()=>{const e=(0,a.useContext)(l);return e||s()}},75813:function(e,t,n){n.d(t,{v:function(){return M}});var o=n(31014),a=n(68048),i=n(16934),s=n(24759),l=n(11473),r=n(91144),d=n(10939),c=n(2250),m=n(7450),u=n(86443),g=n(26022),p=n(39330),h=n(177),f=n(72314),T=n(83667),y=n(25866),A=n(36432),w=n(50111);const b=e=>{const{experimentIds:t,loggedModelId:n}=e,l=(0,m.tz)(),{data:r,isLoading:d,error:c}=(0,s.Zn)({experimentId:t[0],pageSize:1,limit:1,...n?{filterByLoggedModelId:n}:{}}),{data:u,loading:b}=(0,f.L)({experimentId:t[0]}),I=u,v=(0,T.BH)(null===I||void 0===I?void 0:I.tags),_=(v===y.Gk.GENAI_DEVELOPMENT||y.Gk.GENAI_DEVELOPMENT_INFERRED,r&&r.length>0),[x,Y]=(0,g.VA)(),C=(0,o.useMemo)((()=>(0,h.p)(l)),[l]),S=(0,w.Y)(i.B,{componentId:"traces-v3-empty-state-button",onClick:()=>Y({startTimeLabel:"ALL"}),children:(0,w.Y)(m.sA,{id:"WpD2MA",defaultMessage:"View All"})});if(d||b)return(0,w.Y)(w.FK,{children:[...Array(10).keys()].map((e=>(0,w.Y)(a.ParagraphSkeleton,{label:"Loading...",seed:`s-${e}`},e)))});if(c)return(0,w.Y)(a.Empty,{image:(0,w.Y)(i.l,{}),title:(0,w.Y)(m.sA,{id:"ZotI2j",defaultMessage:"Fetching traces failed"}),description:String(c)});if(_){var M;const e=(0,w.Y)(p.S,{}),t=(0,w.Y)(m.sA,{id:"XuzIWs",defaultMessage:'Some traces are hidden by your time range filter: "{filterLabel}"',values:{filterLabel:(0,w.Y)("strong",{children:(null===(M=C.find((e=>e.key===x.startTimeLabel)))||void 0===M?void 0:M.label)||""})}});return(0,w.Y)(a.Empty,{title:(0,w.Y)(m.sA,{id:"Dhr3pC",defaultMessage:"No traces found"}),description:t,button:S,image:e})}return(0,w.Y)(A.i,{baseComponentId:"mlflow.traces"})};var I=n(27288),v=n(48624);var _={name:"1dd35ky",styles:"display:flex;flex:1;overflow:hidden"},x={name:"1dd35ky",styles:"display:flex;flex:1;overflow:hidden"},Y={name:"db3fum",styles:"display:flex;flex-direction:column;width:100%;gap:8px;padding:16px"},C={name:"1w1qal4",styles:"display:flex;align-items:center;justify-content:center;width:100%;height:100%"},S={name:"1nxh63r",styles:"overflow-y:hidden;height:100%;display:flex;flex-direction:column"};const M=o.memo((({experimentId:e,endpointName:t,timeRange:n,loggedModelId:p})=>{const h=(0,l.N9)(),f=(0,m.tz)(),T=(0,r.rE)(),{assessmentInfos:y,allColumns:A,totalCount:M,isLoading:L,error:k,isEmpty:E,tableFilterOptions:P}=(0,s.KW)({experimentId:e,timeRange:n,filterByLoggedModelId:p}),[O,D]=(0,o.useState)(""),[N,F]=(0,s.R7)(),R=(0,I.jE)(),U=(0,o.useCallback)((e=>e.filter((e=>e.type===s.$6.ASSESSMENT||e.type===s.$6.EXPECTATION||e.type===s.$6.INPUT||e.type===s.$6.TRACE_INFO&&[s.tj,s.Rl,s.Te,s.$W,s.YO].includes(e.id)||e.type===s.$6.INTERNAL_MONITOR_REQUEST_TIME))),[]),{selectedColumns:V,toggleColumns:q,setSelectedColumns:H}=(0,s.K0)(e,A,U),[K,$]=(0,s.GY)(V,{key:s.Te,type:s.$6.TRACE_INFO,asc:!1}),{isInitialTimeFilterLoading:W}=(({experimentId:e,isTracesEmpty:t,isTraceMetadataLoading:n})=>{const[o]=(0,v.ok)(),[a,i]=(0,g.VA)(),l=t&&!n&&!o.has(g.vF),{data:r,isLoading:d}=(0,s.Zn)({experimentId:e,tableSort:{key:s.Te,type:s.$6.TRACE_INFO,asc:!1},disabled:!l,limit:500,pageSize:500});l&&r&&r.length>0&&!d&&i({startTimeLabel:"CUSTOM",startTime:r[r.length-1].request_time,endTime:(new Date).toISOString()});return{isInitialTimeFilterLoading:l&&d}})({experimentId:e,isTracesEmpty:E,isTraceMetadataLoading:L}),{data:G,isLoading:B,error:z}=(0,s.Zn)({experimentId:e,currentRunDisplayName:t,searchQuery:O,filters:N,timeRange:n,filterByLoggedModelId:p,tableSort:K}),j=(0,d.C)(),{showEditTagsModalForTrace:Q,EditTagsModal:Z}=(0,c.$)({onSuccess:()=>(0,s.BL)({queryClient:R}),existingTagKeys:(0,s.d9)(G||[]),useV3Apis:!0}),J=(0,o.useMemo)((()=>({deleteTracesAction:{deleteTraces:(e,t)=>j.mutateAsync({experimentId:e,traceRequestIds:t})},exportToEvals:{exportToEvalsInstanceEnabled:!0,getTrace:u.U},editTags:{showEditTagsModalForTrace:Q,EditTagsModal:Z}})),[j,Q,Z]),X=(0,o.useMemo)((()=>({currentCount:null===G||void 0===G?void 0:G.length,logCountLoading:B,totalCount:M,maxAllowedCount:(0,s.pR)()})),[G,M,B]),ee=B||W||L,te=z||k,ne=E&&!ee&&!te;return!T&&ne?(0,w.Y)(b,{experimentIds:[e],loggedModelId:p}):(0,w.Y)(s.sG,{children:(0,w.FD)("div",{css:S,children:[(0,w.Y)(s.w_,{experimentId:e,searchQuery:O,setSearchQuery:D,filters:N,setFilters:F,assessmentInfos:y,traceInfos:G,tableFilterOptions:P,countInfo:X,traceActions:J,tableSort:K,setTableSort:$,allColumns:A,selectedColumns:V,toggleColumns:q,setSelectedColumns:H,isMetadataLoading:L,metadataError:k}),!T&&ne?(0,w.Y)(b,{experimentIds:[e],loggedModelId:p}):(0,w.Y)("div",{css:_,children:(0,w.Y)("div",{css:x,children:ee?(0,w.Y)("div",{css:Y,children:[...Array(10).keys()].map((e=>(0,w.Y)(a.ParagraphSkeleton,{label:"Loading...",seed:`s-${e}`},e)))}):te?(0,w.Y)("div",{css:C,children:(0,w.Y)(a.Empty,{image:(0,w.Y)(i.l,{}),title:f.formatMessage({id:"Bcr4hl",defaultMessage:"Fetching traces failed"}),description:te.message})}):(0,w.Y)(s.tU,{makeHtml:h,children:(0,w.Y)(s._p,{experimentId:e,allColumns:A,currentTraceInfoV3:G||[],currentRunDisplayName:t,getTrace:u.U,assessmentInfos:y,setFilters:F,filters:N,selectedColumns:V,tableSort:K})})})})]})})}))}}]);
//# sourceMappingURL=5813.33361779.chunk.js.map